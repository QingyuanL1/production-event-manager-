"""
æ—¥å¿—åˆ†æå™¨
Log Analyzer

åˆ†æç³»ç»Ÿæ—¥å¿—ï¼Œæä¾›ç»Ÿè®¡ä¿¡æ¯å’Œé”™è¯¯æŠ¥å‘Š
"""

import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from collections import defaultdict, Counter


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨ï¼Œç”¨äºåˆ†æå’Œç»Ÿè®¡ç³»ç»Ÿæ—¥å¿—"""
    
    def __init__(self, log_dir: str = "logs"):
        """
        åˆå§‹åŒ–æ—¥å¿—åˆ†æå™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
        """
        self.log_dir = log_dir
        self.log_pattern = re.compile(r'\[(\d{2}:\d{2}:\d{2})\]\s*(?:ğŸ”|â„¹ï¸|âš ï¸|âŒ|ğŸš¨)?\s*(\w+):\s*(.*)')
    
    def get_log_files(self, date_filter: Optional[str] = None) -> List[str]:
        """
        è·å–æ—¥å¿—æ–‡ä»¶åˆ—è¡¨
        
        Args:
            date_filter: æ—¥æœŸè¿‡æ»¤å™¨ (YYYY-MM-DDæ ¼å¼)
            
        Returns:
            æ—¥å¿—æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not os.path.exists(self.log_dir):
            return []
        
        log_files = []
        for filename in os.listdir(self.log_dir):
            if filename.endswith('.log'):
                if date_filter is None or date_filter in filename:
                    log_files.append(os.path.join(self.log_dir, filename))
        
        return sorted(log_files)
    
    def parse_log_file(self, file_path: str) -> List[Dict]:
        """
        è§£æå•ä¸ªæ—¥å¿—æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            
        Returns:
            è§£æåçš„æ—¥å¿—æ¡ç›®åˆ—è¡¨
        """
        log_entries = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    match = self.log_pattern.match(line)
                    if match:
                        time_str, level, message = match.groups()
                        
                        entry = {
                            'file': os.path.basename(file_path),
                            'line_number': line_num,
                            'time': time_str,
                            'level': level,
                            'message': message,
                            'raw_line': line
                        }
                        log_entries.append(entry)
        
        except Exception as e:
            print(f"Error parsing log file {file_path}: {e}")
        
        return log_entries
    
    def analyze_logs(self, date_filter: Optional[str] = None) -> Dict:
        """
        åˆ†ææ—¥å¿—æ–‡ä»¶ï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            date_filter: æ—¥æœŸè¿‡æ»¤å™¨
            
        Returns:
            æ—¥å¿—åˆ†ææŠ¥å‘Š
        """
        log_files = self.get_log_files(date_filter)
        all_entries = []
        
        # è§£ææ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        for file_path in log_files:
            entries = self.parse_log_file(file_path)
            all_entries.extend(entries)
        
        # ç»Ÿè®¡åˆ†æ
        level_counts = Counter(entry['level'] for entry in all_entries)
        hourly_distribution = defaultdict(int)
        error_messages = []
        warning_messages = []
        
        for entry in all_entries:
            # æŒ‰å°æ—¶åˆ†å¸ƒç»Ÿè®¡
            hour = entry['time'][:2]
            hourly_distribution[hour] += 1
            
            # æ”¶é›†é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯
            if entry['level'] == 'ERROR':
                error_messages.append(entry)
            elif entry['level'] == 'WARNING':
                warning_messages.append(entry)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'summary': {
                'total_entries': len(all_entries),
                'files_analyzed': len(log_files),
                'analysis_date': datetime.now().isoformat(),
                'date_filter': date_filter
            },
            'level_distribution': dict(level_counts),
            'hourly_distribution': dict(hourly_distribution),
            'errors': error_messages[-10:],  # æœ€è¿‘10ä¸ªé”™è¯¯
            'warnings': warning_messages[-10:],  # æœ€è¿‘10ä¸ªè­¦å‘Š
            'files': [os.path.basename(f) for f in log_files]
        }
        
        return report
    
    def find_lca_events(self, date_filter: Optional[str] = None) -> List[Dict]:
        """
        æŸ¥æ‰¾LCAäº‹ä»¶ç›¸å…³çš„æ—¥å¿—æ¡ç›®
        
        Args:
            date_filter: æ—¥æœŸè¿‡æ»¤å™¨
            
        Returns:
            LCAäº‹ä»¶æ—¥å¿—åˆ—è¡¨
        """
        log_files = self.get_log_files(date_filter)
        lca_events = []
        
        lca_keywords = ['LCA', 'DOS', 'äº§é‡æŸå¤±', 'è¡¥å¿äº§é‡', 'ç­æ¬¡æ£€æŸ¥']
        
        for file_path in log_files:
            entries = self.parse_log_file(file_path)
            
            for entry in entries:
                if any(keyword in entry['message'] for keyword in lca_keywords):
                    lca_events.append(entry)
        
        return lca_events
    
    def generate_daily_report(self, date: Optional[str] = None) -> str:
        """
        ç”Ÿæˆæ¯æ—¥æ—¥å¿—æŠ¥å‘Š
        
        Args:
            date: æ—¥æœŸ (YYYY-MM-DDæ ¼å¼)ï¼Œé»˜è®¤ä¸ºä»Šå¤©
            
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        report = self.analyze_logs(date)
        
        report_lines = [
            f"ğŸ“Š æ—¥å¿—åˆ†ææŠ¥å‘Š - {date}",
            "=" * 50,
            f"ğŸ“„ åˆ†ææ–‡ä»¶æ•°: {report['summary']['files_analyzed']}",
            f"ğŸ“ æ€»æ—¥å¿—æ¡ç›®: {report['summary']['total_entries']}",
            "",
            "ğŸ“ˆ æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:",
        ]
        
        for level, count in report['level_distribution'].items():
            report_lines.append(f"  {level}: {count}")
        
        if report['errors']:
            report_lines.extend([
                "",
                "âŒ æœ€è¿‘é”™è¯¯ (æœ€å¤š10æ¡):",
            ])
            for error in report['errors']:
                report_lines.append(f"  [{error['time']}] {error['message']}")
        
        if report['warnings']:
            report_lines.extend([
                "",
                "âš ï¸ æœ€è¿‘è­¦å‘Š (æœ€å¤š10æ¡):",
            ])
            for warning in report['warnings']:
                report_lines.append(f"  [{warning['time']}] {warning['message']}")
        
        report_lines.extend([
            "",
            "ğŸ“ åˆ†æçš„æ—¥å¿—æ–‡ä»¶:",
        ])
        for file_name in report['files']:
            report_lines.append(f"  - {file_name}")
        
        return "\n".join(report_lines)
    
    def export_report(self, output_file: str, date_filter: Optional[str] = None):
        """
        å¯¼å‡ºæ—¥å¿—åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            date_filter: æ—¥æœŸè¿‡æ»¤å™¨
        """
        report = self.analyze_logs(date_filter)
        
        try:
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            print(f"å¯¼å‡ºæŠ¥å‘Šå¤±è´¥: {e}")